{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA typical RAG application has two main components:\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\n\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\nThe most common full sequence from raw data to answer looks like:\\n\\nIndexing\\n1) Load: First we need to load our data. This is done with Document Loaders.\\n2) Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\\n3) Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\\n\\nRetrieval and generation\\n4) Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\n5) Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A typical RAG application has two main components:\n",
    "\n",
    "Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:\n",
    "\n",
    "Indexing\n",
    "1) Load: First we need to load our data. This is done with Document Loaders.\n",
    "2) Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3) Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "Retrieval and generation\n",
    "4) Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "5) Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Device: cuda\n"
     ]
    }
   ],
   "source": [
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"My Device: {}\".format(my_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model is loading\n"
     ]
    }
   ],
   "source": [
    "## initialize our peft-tuned LLM\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M\" # HuggingFaceTB/SmolLM2-135M, HuggingFaceTB/SmolLM2-360M, HuggingFaceTB/SmolLM2-1.7B, HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(my_device)\n",
    "\n",
    "\n",
    "# initialize the peft model\n",
    "print(\"PEFT Model is loading\")\n",
    "peft_model_path = 'tuned_models_SmolLM2/bank_qa_base_tune_peft-2025_02_25_22_13_1e-3_expanded/checkpoint-1440'\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path).to(my_device)\n",
    "peft_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What phones are able to use the mobile app?</td>\n",
       "      <td>Any phone that has access to Google, Apple or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I view &amp; print statements, notices and ...</td>\n",
       "      <td>To view e-statements, notices &amp; tax documents:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I send or receive a wire transfer?</td>\n",
       "      <td>FSB offers several wire transfer options.\\n\\nD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is your routing number?</td>\n",
       "      <td>The routing number is 073908045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I protect myself from Scammers?</td>\n",
       "      <td>Fraudulent calls, texts, and emails are on the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions  \\\n",
       "0        What phones are able to use the mobile app?   \n",
       "1  How do I view & print statements, notices and ...   \n",
       "2          How do I send or receive a wire transfer?   \n",
       "3                       What is your routing number?   \n",
       "4             How do I protect myself from Scammers?   \n",
       "\n",
       "                                             Answers  \n",
       "0  Any phone that has access to Google, Apple or ...  \n",
       "1  To view e-statements, notices & tax documents:...  \n",
       "2  FSB offers several wire transfer options.\\n\\nD...  \n",
       "3                    The routing number is 073908045  \n",
       "4  Fraudulent calls, texts, and emails are on the...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initalize external document\n",
    "external_df = pd.read_excel('fsb1879_BankingQAs.xlsx')\n",
    "\n",
    "# drop nan values\n",
    "external_df.dropna(inplace=True)\n",
    "# drop duplicates\n",
    "external_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(external_df.shape)\n",
    "\n",
    "external_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Formats the query based on the instruction tuning prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the updated instruction-tuned prompt template\n",
    "    formatted_prompt = f\"\"\"\n",
    "    ### Instruction:\n",
    "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "    ### Customer Request:\n",
    "    {query_text}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode_plus(formatted_prompt, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def tune_generate_output(my_inputs, my_tokenizer, my_model, max_tokens=50, temp=0.3, top_p=0.9, top_k=50, penalty_score=1.2, do_sample=True, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the input.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = my_model.generate(\n",
    "                \n",
    "        input_ids=my_inputs[\"input_ids\"],  # Pass input_ids\n",
    "        attention_mask=my_inputs[\"attention_mask\"],  # Pass attention mask\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=penalty_score,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=my_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode output and clean it up\n",
    "    output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure safe parsing without hardcoded token removal\n",
    "    cleaned_output_text = output_text.strip()\n",
    "\n",
    "    return cleaned_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Question: \n",
      "What phones are able to use the mobile app?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "Human Response: \n",
      "Any phone that has access to Google, Apple or Samsung app stores can download the FSB Mobile app.\n",
      "\n",
      "At minimum the device must run on iOS 15 or Android 10.\n",
      "\n",
      "To ensure the highest level of security, we recommend always updating your device to the latest operating system and app release. The simplest way to achieve this, is to set automatic updates in your device settings. For further support on your device, contact your phone manufacturer's customer support for assistance.\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "LLM Response: \n",
      "I can provide you with information about which mobile apps allow users to access our services conveniently through their respective devices or platforms. To get started, here is what we recommend for using your preferred application on various smartphones/tablets:\n",
      "\n",
      "1. Mobile Banking App: If you prefer convenience over security features like online transactions or transaction history management, consider checking out our dedicated mobile banking app. It offers real-time updates of account balances while providing easy accessibility via push notifications when needed.\n",
      "2. Social Media Apps: Platforms such as Facebook Messenger (for messaging), WhatsApp (video calls) or Instagram Live Chat enable seamless communication between friends who may be interested in sharing financial data publicly. These channels often include tools that help manage accounts easily without needing direct interaction from us directly.\n",
      "3. Online Payment Services: Websites offering online payment options have become increasingly popular among consumers due to convenient methods of making purchases securely. Some notable examples include {{Company Name}}'s official website where you can pay bills, transfer funds, or view upcoming payments seamlessly.\n",
      "4. Digital Health Monitoring Tools: Devices equipped with biometric authentication systems offer secure monitoring capabilities similar to those found within medical applications designed specifically for healthcare purposes. Examples range from fitness trackers tracking physical activity levels to smart home assistants managing lighting settings based on occupancy patterns.\n",
      "5. Educational Resources: Various educational institutions utilize digital learning resources available across multiple platforms including websites, eBooks, videos, podcasts, etc., allowing students worldwide to engage meaningfully during remote classes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "sample_question = external_df.iloc[index,0]\n",
    "sample_response = external_df.iloc[index,1]\n",
    "\n",
    "sample_inputs = tune_generate_chat_template(sample_question, tokenizer, my_device)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=300, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                    do_sample = True, instruct_model=False)\n",
    "\n",
    "sample_output_wout_prompt = sample_output.split(\"Response:\")[1].strip()\n",
    "\n",
    "\n",
    "dash_line = '-'*10\n",
    "print('Human Question: \\n{}\\n'.format(sample_question))\n",
    "print(dash_line)\n",
    "print('Human Response: \\n{}\\n'.format(sample_response)) # better reading: textwrap.fill(sample_response, width=150)\n",
    "print(dash_line)\n",
    "print('LLM Response: \\n{}\\n'.format(sample_output_wout_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480baa044fe04ad48decb54b45d98c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367750411b4a449798e34b86f05d64b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9639bca222a493e9ad05e26052887bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e261a3df6764acf91b17948636b3223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d164326c34c419389c374dc3220b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14c45df148348e58c2a387eaea1921f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e807eae8af45d5ab97ca612f1beae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906944b762074584886861616a0254d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0eb696b9794c9796c2e0095bb0a745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3b8844046a41f4b7557f31fbfe0f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faec6e75944447c0abc9a03ed950ce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Embedder that will be used during RAG embedding and retrieval\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"paraphrase-multilingual-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate local vector db over the questions\n",
    "## then in the retrieval stage, the index of the retrieval function will be passed this df\n",
    "## then corresponding answer will be returned to the prompt\n",
    "local_vector_db_question = DocArrayInMemorySearch.from_texts(\n",
    "    external_df[\"Questions\"].tolist(), embedder\n",
    ")\n",
    "\n",
    "## retriever from the local vector db\n",
    "retriever = local_vector_db_question.as_retriever(\n",
    "    search_kwargs={\"score_threshold\": 0.3, \"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relavent Documents: [Document(page_content='What phones are able to use the mobile app?'), Document(page_content='Can I use Touch Authentication on my mobile device?')]\n"
     ]
    }
   ],
   "source": [
    "# Sample Question and its relation with the \"User Documents\"\n",
    "relevant_documents = retriever.get_relevant_documents(query='Can i use my IOS mobile phone?')\n",
    "print(\"Relavent Documents: {}\".format(relevant_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_rag_generate_chat_template(query_text, external_info, my_tokenizer, my_device, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Formats the query based on the instruction tuning prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the updated instruction-tuned prompt template\n",
    "    formatted_prompt = f\"\"\"\n",
    "    ### Instruction:\n",
    "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "    ### External Info:\n",
    "    {external_info}\n",
    "\n",
    "    ### Customer Request:\n",
    "    {query_text}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode_plus(formatted_prompt, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: \n",
      "Can i use my IOS mobile phone?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "External Document Relevant Question: \n",
      "What phones are able to use the mobile app?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "External Document Relevant Answer: \n",
      "Any phone that has access to Google, Apple or Samsung app stores can download the FSB Mobile app.\n",
      "\n",
      "At minimum the device must run on iOS 15 or Android 10.\n",
      "\n",
      "To ensure the highest level of security, we recommend always updating your device to the latest operating system and app release. The simplest way to achieve this, is to set automatic updates in your device settings. For further support on your device, contact your phone manufacturer's customer support for assistance.\n",
      "\n",
      "\n",
      "RAG-Supported LLM Answer: \n",
      "### Instruction:\n",
      "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "    ### External Info:\n",
      "    Any phone that has access to Google, Apple or Samsung app stores can download the FSB Mobile app.\n",
      "\n",
      "At minimum the device must run on iOS 15 or Android 10.\n",
      "\n",
      "To ensure the highest level of security, we recommend always updating your device to the latest operating system and app release. The simplest way to achieve this, is to set automatic updates in your device settings. For further support on your device, contact your phone manufacturer's customer support for assistance.\n",
      "\n",
      "\n",
      "    ### Customer Request:\n",
      "    Can i use my IOS mobile phone?\n",
      "\n",
      "    ### Response:\n",
      "     Yes, you certainly can! We have designed our {{FBS Branch}} application specifically with mobile devices like smartphones and tablets into account. It supports all major platforms including iOS (Apple) and Android (Google).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Can i use my IOS mobile phone?\"\n",
    "\n",
    "retrieve_most_relevant_question = retriever.get_relevant_documents(query=sample_query)[0]\n",
    "relevant_question_text = retrieve_most_relevant_question.page_content\n",
    "relevant_question_index = external_df[external_df['Questions'] == relevant_question_text].index[0]\n",
    "relevant_question_answer_text = external_df['Answers'].iloc[relevant_question_index]\n",
    "\n",
    "print(\"User Query: \\n{}\\n\".format(sample_query))\n",
    "print(dash_line)\n",
    "print(\"External Document Relevant Question: \\n{}\\n\".format(relevant_question_text))\n",
    "print(dash_line)\n",
    "print(\"External Document Relevant Answer: \\n{}\\n\".format(relevant_question_answer_text))\n",
    "print(dash_line)\n",
    "## now append the answer to the prompt and try to get meaningful response\n",
    "sample_inputs = tune_rag_generate_chat_template(sample_query, relevant_question_answer_text, tokenizer, my_device, instruct_model=False)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=300, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                    do_sample = True, instruct_model=False)\n",
    "\n",
    "\n",
    "print(\"RAG-Supported LLM Answer: \\n{}\\n\".format(sample_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: \n",
      "How can i be apart from scammers?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "External Document Relevant Question: \n",
      "How do I protect myself from Scammers?\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "External Document Relevant Answer: \n",
      "Fraudulent calls, texts, and emails are on the rise. These scammers are very good at what they do, and will seem very convincing to gain access to your financial information. You may think that you are not the type of person to be easily duped, but these fraudsters are pros.\n",
      "\n",
      "If you ever receive a call, text or email claiming to be an employee of FSB, know that we will NEVER ask for any of the following information:\n",
      "\n",
      "Password\n",
      "Username\n",
      "Card PIN\n",
      "Account numbers\n",
      "If someone calls you, or you have clicked on a link in a text or email that asks for this information, call FSB immediately. We will assist you with changing your username and password, and hopefully prevent any fraudulent charges.\n",
      "\n",
      "Our card providers, Shazam and FIS, are also monitoring your debit and credit card use 24/7. You may receive a call or text from them to verify a suspicious transaction. They will also NEVER ask for any of the information listed above. They should have no access to your bank account information, only transactions made on their cards.\n",
      "\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "RAG-Supported LLM Answer: \n",
      "### Instruction:\n",
      "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "    ### External Info:\n",
      "    Fraudulent calls, texts, and emails are on the rise. These scammers are very good at what they do, and will seem very convincing to gain access to your financial information. You may think that you are not the type of person to be easily duped, but these fraudsters are pros.\n",
      "\n",
      "If you ever receive a call, text or email claiming to be an employee of FSB, know that we will NEVER ask for any of the following information:\n",
      "\n",
      "Password\n",
      "Username\n",
      "Card PIN\n",
      "Account numbers\n",
      "If someone calls you, or you have clicked on a link in a text or email that asks for this information, call FSB immediately. We will assist you with changing your username and password, and hopefully prevent any fraudulent charges.\n",
      "\n",
      "Our card providers, Shazam and FIS, are also monitoring your debit and credit card use 24/7. You may receive a call or text from them to verify a suspicious transaction. They will also NEVER ask for any of the information listed above. They should have no access to your bank account information, only transactions made on their cards.\n",
      "\n",
      "\n",
      "    ### Customer Request:\n",
      "    How can i be apart from scammers?\n",
      "\n",
      "    ### Response:\n",
      "     I'm sorry to hear about the concerns regarding potential scams. It is important to remain vigilant when interacting online. Here are some tips to help protect yourself against such threats:\n",
      "\n",
      "1. Keep your personal information secure: Avoid sharing sensitive details like passwords, usernames, or accounts over social media platforms or messaging apps where it could potentially fall into the wrong hands.\n",
      "\n",
      "2. Be cautious of unsolicited communications: If anyone contacts you via phone, mail, or email asking for specific login credentials, don't hesitate to report them as soon as possible so that proper action can take place. Remember, if there was reason to suspect phishing attempts before, then now would be the best time to investigate further.\n",
      "\n",
      "3. Secure your devices: Regularly update your operating systems, antivirus software, and other security tools associated with your device(s). This helps ensure better protection against unauthorized intrusions while using those gadgets.\n",
      "\n",
      "4. Stay informed: Follow up regularly with updates provided by our team concerning recent developments within the industry. Knowledgeable staff members who understand current trends make great resources during times of uncertainty.\n",
      "\n",
      "5. Report suspected incidents promptly: Should you discover anything unusual involving your finances, wallet contents, or communication channels, reach out directly to me through {{Customer Support Phone Number}} or visit our website at {{Company Website URL}}, preferably after investigating thoroughly. Our dedicated support teams work diligently behind the scenes to resolve issues efficiently and provide guidance based on accurate facts rather than speculation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"How can i be apart from scammers?\"\n",
    "\n",
    "retrieve_most_relevant_question = retriever.get_relevant_documents(query=sample_query)[0]\n",
    "relevant_question_text = retrieve_most_relevant_question.page_content\n",
    "relevant_question_index = external_df[external_df['Questions'] == relevant_question_text].index[0]\n",
    "relevant_question_answer_text = external_df['Answers'].iloc[relevant_question_index]\n",
    "\n",
    "print(\"User Query: \\n{}\\n\".format(sample_query))\n",
    "print(dash_line)\n",
    "print(\"External Document Relevant Question: \\n{}\\n\".format(relevant_question_text))\n",
    "print(dash_line)\n",
    "print(\"External Document Relevant Answer: \\n{}\\n\".format(relevant_question_answer_text))\n",
    "print(dash_line)\n",
    "\n",
    "## now append the answer to the prompt and try to get meaningful response\n",
    "sample_inputs = tune_rag_generate_chat_template(sample_query, relevant_question_answer_text, tokenizer, my_device, instruct_model=False)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=300, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                    do_sample = True, instruct_model=False)\n",
    "\n",
    "\n",
    "print(\"RAG-Supported LLM Answer: \\n{}\\n\".format(sample_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
