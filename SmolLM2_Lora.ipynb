{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vIOXmw-gEejK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GenerationConfig, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B54sv3Y3FJ4G"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_K5UgwXZJIvT"
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6PXn6KNEnOg",
    "outputId": "9ff4252b-f873-4c9e-c053-633a26eab20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Device: cuda\n"
     ]
    }
   ],
   "source": [
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"My Device: {}\".format(my_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lnQPpxCCFLY1"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-360M\" # HuggingFaceTB/SmolLM2-135M, HuggingFaceTB/SmolLM2-360M, HuggingFaceTB/SmolLM2-1.7B, HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YD5VxVucFSZW"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KFgYs3_2FjLp"
   },
   "outputs": [],
   "source": [
    "def generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
    "    # create a system message\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": query_text}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode(query_text, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def generate_output(my_inputs, my_tokenizer, my_model, max_tokens = 50, temp = 0.3, top_p = 0.9, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False):\n",
    "\n",
    "    if instruct_model:\n",
    "        outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample)\n",
    "        output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output_text = output_text.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
    "    else:\n",
    "         outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample,\n",
    "                                     eos_token_id=my_tokenizer.eos_token_id)\n",
    "         cleaned_output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return cleaned_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Formats the query based on the instruction tuning prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the updated instruction-tuned prompt template\n",
    "    formatted_prompt = f\"\"\"\n",
    "    ### Instruction:\n",
    "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "    ### Customer Request:\n",
    "    {query_text}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def tune_generate_output(my_inputs, my_tokenizer, my_model, max_tokens=50, temp=0.3, top_p=0.9, top_k=50, penalty_score=1.2, do_sample=True, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the input.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = my_model.generate(\n",
    "        my_inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=penalty_score,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=my_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode output and clean it up\n",
    "    output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure safe parsing without hardcoded token removal\n",
    "    cleaned_output_text = output_text.strip()\n",
    "\n",
    "    return cleaned_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60R41YyWFrnu",
    "outputId": "1a7dc468-69d4-43e4-b524-d724a8b4e4a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Germany?\n",
      "==========\n",
      "Question: What is the capital of Germany?\n",
      "Answer: Berlin.\n",
      "\n",
      "2016-08-31 09:\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"Question: What is the capital of Germany?\"\n",
    "sample_inputs = generate_chat_template(sample_query, tokenizer, my_device)\n",
    "sample_output = generate_output(sample_inputs, tokenizer, model, max_tokens=20, temp = 0.6, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
    "\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbtOydeiFsxc",
    "outputId": "5f17705f-8cae-47ca-ed35-ce2cd0d451b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tags', 'instruction', 'category', 'intent', 'response'],\n",
      "    num_rows: 25545\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## DATASET ##\n",
    "huggingface_dataset_name = \"bitext/Bitext-retail-banking-llm-chatbot-training-dataset\"\n",
    "\n",
    "ds = load_dataset(huggingface_dataset_name, split='train')\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_test_split = ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "train_validation_split = train_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_validation_split['train']\n",
    "validation_dataset = train_validation_split['test']\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxKUJZeeFy4N",
    "outputId": "0266da0f-dfbe-4f0d-8eea-93861f806654",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Question: help me disputing a withdrawal\n",
      "==========\n",
      "I'm sorry to hear that you need assistance with disputing a withdrawal. I'll do my best to help you with that. Here's what you can do:\n",
      "\n",
      "1. Check your account statement: Take a look at your recent transactions to gather all the necessary information about the withdrawal, such as the date, time, and location.\n",
      "\n",
      "2. Contact your bank: Reach out to your bank's customer service department through their helpline or visit the nearest branch. Provide them with the details of the withdrawal and explain why you believe there is an issue. They will guide you further in the dispute process and may require additional information or documentation.\n",
      "\n",
      "3. File a dispute claim: Your bank will assist you in filing a formal dispute claim. They will investigate the transaction and work towards a resolution. Make sure to provide them with any evidence or supporting documents related to the disputed withdrawal.\n",
      "\n",
      "4. Monitor your account: Until the dispute is resolved, keep an eye on your account for any additional transactions or updates from your bank. They will keep you informed about the progress of the investigation and provide any necessary updates.\n",
      "\n",
      "Remember, each bank may have different dispute resolution processes, so it's important to follow their instructions and provide any requested information promptly. If you have any further questions or need additional assistance, feel free to ask.\n",
      "==========\n",
      "Question: help me disputing a withdrawal of the 1973 US-Israeli agreement on Palestinian refugees.\n",
      "\n",
      "Answer: The United States and Israel have made it clear that they will not negotiate with Palestinians until there is an end to their occupation, which includes all aspects from\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 10\n",
    "\n",
    "sample_query = \"Question: \" + train_dataset['instruction'][sample_idx]\n",
    "sample_response = train_dataset['response'][sample_idx]\n",
    "sample_inputs = generate_chat_template(sample_query, tokenizer, my_device)\n",
    "sample_output = generate_output(sample_inputs, tokenizer, model, max_tokens=50, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_response)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feZg4-rxF3aw",
    "outputId": "53d8dda1-6768-4754-9cc7-01654b7484c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 361821120\n",
      "all model parameters: 361821120\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 960)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8D-ODI2XJJxy",
    "outputId": "31dc2824-2f85-4fd4-e88d-2ab38db86475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 1572864\n",
      "all model parameters: 1712949248\n",
      "percentage of trainable model parameters: 0.09%\n"
     ]
    }
   ],
   "source": [
    "## LORA ##\n",
    "lora_config = LoraConfig(\n",
    "    r=4, # Rank\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], #\"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config).to(my_device)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS3kxeoaJUMM",
    "outputId": "19cb0610-81b3-4fc5-d109-5d60c6f07f91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "where do I see the card annul fee?\n",
      "==========\n",
      "I'll do my best! I'm here to help you with that. To view the annual fee for your card, you can follow these steps:\n",
      "\n",
      "1. Log in to your online banking or mobile banking app.\n",
      "2. Navigate to the \"Accounts\" or \"Credit Cards\" section.\n",
      "3. Look for your specific credit card and click on it.\n",
      "4. You should find the annual fee listed in the card details or account information.\n",
      "\n",
      "If you're having trouble locating the annual fee or need further assistance, please don't hesitate to let me know.\n",
      "==========\n",
      "\n",
      "### Instruction:\n",
      "You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "### Customer Request:\n",
      "where do I see the card annul fee?\n",
      "\n",
      "### Response:\n",
      "Thank you for your inquiry! The annual charge on credit cards is typically between $0-$15, but it can vary depending on factors such as interest rates or rewards programs offered by the bank. It would be best if you could check with your\n"
     ]
    }
   ],
   "source": [
    "## TRY DIFFERENT PROMPTS ##\n",
    "sample_idx = 100\n",
    "\n",
    "sample_query = train_dataset['instruction'][sample_idx]\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "### Customer Request:\n",
    "{sample_query}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(my_device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.6, top_p=0.7, top_k=50, repetition_penalty=1.2, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "sample_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_response = train_dataset['response'][sample_idx]\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_response)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2iSmuPCPGGc",
    "outputId": "490fe3e0-3631-4516-ca37-033a5c5bd4b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "where do I see the card annul fee?\n",
      "==========\n",
      "I'll do my best! I'm here to help you with that. To view the annual fee for your card, you can follow these steps:\n",
      "\n",
      "1. Log in to your online banking or mobile banking app.\n",
      "2. Navigate to the \"Accounts\" or \"Credit Cards\" section.\n",
      "3. Look for your specific credit card and click on it.\n",
      "4. You should find the annual fee listed in the card details or account information.\n",
      "\n",
      "If you're having trouble locating the annual fee or need further assistance, please don't hesitate to let me know.\n",
      "==========\n",
      "### Instruction:\n",
      "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "    ### Customer Request:\n",
      "    where do I see the card annul fee?\n",
      "\n",
      "    ### Response:\n",
      "    10% of your credit limit is charged as card annual fee, if you have not used any balance on this account for last 6 months then there will be no charge at all.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 100\n",
    "\n",
    "sample_query = train_dataset['instruction'][sample_idx]\n",
    "sample_response = train_dataset['response'][sample_idx]\n",
    "\n",
    "sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, model, max_tokens=100, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False)\n",
    "\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_response)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "eb906c87227e4037a34cf5f8dc1831df",
      "2978b28661a54b21b955c9e5a7683890",
      "7ec1a9fdb51d4d18867616f23c624286",
      "f4e9d6641c99418aaa94eb1d165e5a03",
      "fbb6fbc83cf8483db03bc61fdaf6b82d",
      "5137e29592024268a2841c20e71f52e5",
      "1f868c0d90ab4c3a98badb434ff3ec62",
      "8519dd47389f487fbc9f1103f7d19766",
      "7bf4c685ff574a6a8a68417d472f30ba",
      "b1cc2717ce364684b17f9ad75ae6e0bd",
      "8115ab5fbaf24298a3d730da1ae4727b"
     ]
    },
    "id": "MRUppcxGPixE",
    "outputId": "6a13c859-79da-4175-f4ca-59b03e6b4fa2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23053/23053 [00:07<00:00, 3150.32 examples/s]\n",
      "Map: 100%|██████████| 1278/1278 [00:00<00:00, 3167.92 examples/s]\n",
      "Map: 100%|██████████| 1214/1214 [00:00<00:00, 3327.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the dataset using the fine-tuning instruction prompt format.\n",
    "    Handles batched processing by iterating over each example.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the full prompt for each example in the batch\n",
    "    formatted_prompts = [\n",
    "        f\"\"\"\n",
    "        ### Instruction:\n",
    "        You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "        ### Customer Request:\n",
    "        {instruction}\n",
    "\n",
    "        ### Response:\n",
    "        {response}\n",
    "        \"\"\"\n",
    "        for instruction, response in zip(examples[\"instruction\"], examples[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize each formatted prompt\n",
    "    tokenized_outputs = tokenizer(\n",
    "        formatted_prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust as needed\n",
    "    )\n",
    "\n",
    "    # Return tokenized inputs\n",
    "    return {\n",
    "        \"input_ids\": tokenized_outputs[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_outputs[\"attention_mask\"],\n",
    "        \"labels\": tokenized_outputs[\"input_ids\"],  # Causal LM: labels = input_ids\n",
    "    }\n",
    "\n",
    "# Tokenizing and processing all dataset splits\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['tags', 'instruction', 'category', 'intent', 'response'])\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['tags', 'instruction', 'category', 'intent', 'response'])\n",
    "\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_dataset = tokenized_validation_dataset.remove_columns(['tags', 'instruction', 'category', 'intent', 'response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-2YLdvKYiEn",
    "outputId": "61b08275-77cb-4607-d743-76f8e4a4055d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (23053, 3)\n",
      "Validation: (1214, 3)\n",
      "Test: (1278, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23053\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1214\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1278\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## now merge the splits into a single dataset format\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train_dataset,\n",
    "    'validation': tokenized_validation_dataset,\n",
    "    'test': tokenized_test_dataset\n",
    "})\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "WPaNqlYLYk-k",
    "outputId": "680a76b0-3a46-4ef0-8ecd-ee0827b426f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/1440 41:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.998600</td>\n",
       "      <td>0.357347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.305864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.286572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.276604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>0.269064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.263973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.255800</td>\n",
       "      <td>0.259230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.256608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.253669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.251624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.249362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.247617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.246321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.245430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1440, training_loss=0.38027189373970033, metrics={'train_runtime': 2465.9386, 'train_samples_per_second': 18.697, 'train_steps_per_second': 0.584, 'total_flos': 2.2823158784458752e+17, 'train_loss': 0.38027189373970033, 'epoch': 1.9989590562109645})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### PEFT Training ####\n",
    "output_dir = f'tuned_models_SmolLM2/bank_qa_base_tune_peft-17B-{str(time.strftime(\"%Y_%m_%d_%H_%M\"))}'\n",
    "\n",
    "## DO IT FALSE, IF THERE ARE ANY VALUABLE MODELS!!!!\n",
    "if False:\n",
    "  shutil.rmtree('tuned_models_SmolLM2')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "step_size = 100 # we have 11k in total\n",
    "lr = 1e-3\n",
    "n_epochs = 2\n",
    "batch_size = 8\n",
    "gradient_acc = 4\n",
    "warmup_steps = 50 # previously:0\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=False,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=n_epochs,\n",
    "    logging_steps=step_size,  # Logs training loss every x.th steps\n",
    "    save_steps=step_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    logging_strategy=\"steps\",  # Ensures logs are printed every x.th steps\n",
    "    evaluation_strategy=\"steps\",  # Runs validation after each epoch\n",
    "    save_strategy=\"steps\",  # Saves model checkpoints after each epoch\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_acc,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    report_to=\"none\"  # Disable WandB\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load PEFT Model and run for a sentence ####\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "sample_query = test_dataset['instruction'][sample_idx]\n",
    "sample_response = test_dataset['response'][sample_idx]\n",
    "\n",
    "sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=200, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                     do_sample = True, instruct_model=False)\n",
    "\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_response)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = 'tuned_models_SmolLM2/bank_qa_base_tune_peft-2025_02_25_22_48_1e-3_expanded_warmup50/checkpoint-1440/'\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_path).to(my_device)\n",
    "peft_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "I want to recover a swallowed card by an ATM, will you help me?\n",
      "==========\n",
      "I'm here to help you recover your swallowed card from the ATM. I understand how frustrating and worrisome it can be when your card gets stuck in the machine. Here's what you can do to retrieve your card:\n",
      "\n",
      "1. Remain calm and don't panic. This situation is not uncommon, and there are steps you can take to resolve it.\n",
      "2. Contact your bank or credit card issuer immediately. They will be able to assist you further and guide you through the process of recovering your card.\n",
      "3. Provide them with all the necessary details such as the location of the ATM, the date and time of the incident, and any other relevant information they may require.\n",
      "4. Your bank will initiate the retrieval process by contacting the ATM service provider. They will work together to safely retrieve your card and return it to you.\n",
      "5. Keep in mind that the process may take some time, depending on various factors, such as the availability of technicians and the specific ATM involved.\n",
      "6. In the meantime, consider securing an alternate payment method to ensure you can still make necessary transactions while waiting for your card to be recovered.\n",
      "7. If you have any concerns or questions during the process, don't hesitate to reach out to your bank's customer service for further assistance.\n",
      "\n",
      "Please remember to never attempt to retrieve your card yourself by any means, as this can cause further damage to the machine or put your safety at risk. It's always best to follow the proper procedures to ensure a smooth and safe retrieval process.\n",
      "\n",
      "Let me know if there's anything else I can assist you with!\n",
      "==========\n",
      "### Instruction:\n",
      "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "    ### Customer Request:\n",
      "    I want to recover a swallowed card by an ATM, will you help me?\n",
      "\n",
      "    ### Response:\n",
      "     It's regrettable that you're experiencing trouble retrieving your swallowed card from the ATM. Don't worry, I'm here to assist you with this issue. Here's what we can do:\n",
      "\n",
      "1. Contact our {{Customer Support Team}} immediately at {{Customer Support Phone Number}} or through live chat on our website at {{Company Website URL}}. They'll be able to guide you through the process of recovering your card.\n",
      "2. Provide them with all the necessary details such as the location where the incident occurred (ATM number/name), date and time when it happened, any relevant information about the transaction amount, and if possible, take note of other identifying information like the PIN code associated with the card. This helps us track down the specific machine responsible for the theft more efficiently.\n",
      "3. Our team may require some verification steps during the recovery process. They might ask you questions related to recent transactions, account activity history, or even provide additional security measures based on their investigation into\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "sample_query = test_dataset['instruction'][sample_idx]\n",
    "sample_response = test_dataset['response'][sample_idx]\n",
    "\n",
    "sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
    "sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=200, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                     do_sample = True, instruct_model=False)\n",
    "\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(sample_query)\n",
    "print(\"=\"*10)\n",
    "print(sample_response)\n",
    "print(\"=\"*10)\n",
    "print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f868c0d90ab4c3a98badb434ff3ec62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2978b28661a54b21b955c9e5a7683890": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5137e29592024268a2841c20e71f52e5",
      "placeholder": "​",
      "style": "IPY_MODEL_1f868c0d90ab4c3a98badb434ff3ec62",
      "value": "Map: 100%"
     }
    },
    "5137e29592024268a2841c20e71f52e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf4c685ff574a6a8a68417d472f30ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ec1a9fdb51d4d18867616f23c624286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8519dd47389f487fbc9f1103f7d19766",
      "max": 1278,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7bf4c685ff574a6a8a68417d472f30ba",
      "value": 1278
     }
    },
    "8115ab5fbaf24298a3d730da1ae4727b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8519dd47389f487fbc9f1103f7d19766": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1cc2717ce364684b17f9ad75ae6e0bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb906c87227e4037a34cf5f8dc1831df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2978b28661a54b21b955c9e5a7683890",
       "IPY_MODEL_7ec1a9fdb51d4d18867616f23c624286",
       "IPY_MODEL_f4e9d6641c99418aaa94eb1d165e5a03"
      ],
      "layout": "IPY_MODEL_fbb6fbc83cf8483db03bc61fdaf6b82d"
     }
    },
    "f4e9d6641c99418aaa94eb1d165e5a03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1cc2717ce364684b17f9ad75ae6e0bd",
      "placeholder": "​",
      "style": "IPY_MODEL_8115ab5fbaf24298a3d730da1ae4727b",
      "value": " 1278/1278 [00:01&lt;00:00, 1244.37 examples/s]"
     }
    },
    "fbb6fbc83cf8483db03bc61fdaf6b82d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
