{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from peft import PeftModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Device: cuda\n"
     ]
    }
   ],
   "source": [
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"My Device: {}\".format(my_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation type\n",
    "is_fine_tune_evaluation = True # If its False, then the script uses the base model only\n",
    "peft_model_path = 'tuned_models_SmolLM2/bank_qa_base_tune_peft-17B-2025_02_25_23_37_1e-3_warmup50/checkpoint-1440'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model is loading\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-1.7B\" # HuggingFaceTB/SmolLM2-135M, HuggingFaceTB/SmolLM2-360M, HuggingFaceTB/SmolLM2-1.7B, HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(my_device)\n",
    "\n",
    "if is_fine_tune_evaluation:\n",
    "    # initialize the peft model\n",
    "    print(\"PEFT Model is loading\")\n",
    "    peft_model = PeftModel.from_pretrained(model, peft_model_path).to(my_device)\n",
    "    peft_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
    "    # create a system message\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": query_text}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode(query_text, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def generate_output(my_inputs, my_tokenizer, my_model, max_tokens = 50, temp = 0.3, top_p = 0.9, top_k=50, penalty_score=1.2, do_sample = True, instruct_model=False):\n",
    "\n",
    "    if instruct_model:\n",
    "        outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample)\n",
    "        output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_output_text = output_text.split(\"<|im_start|>assistant\")[1].split(\"<|im_end|>\")[0].strip()\n",
    "    else:\n",
    "         outputs = my_model.generate(my_inputs, max_new_tokens=max_tokens, temperature=temp, top_p=top_p, top_k=top_k, repetition_penalty=penalty_score, do_sample=do_sample,\n",
    "                                     eos_token_id=my_tokenizer.eos_token_id)\n",
    "         cleaned_output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return cleaned_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_generate_chat_template(query_text, my_tokenizer, my_device, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Formats the query based on the instruction tuning prompt template.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the updated instruction-tuned prompt template\n",
    "    formatted_prompt = f\"\"\"\n",
    "    ### Instruction:\n",
    "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "    ### Customer Request:\n",
    "    {query_text}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "    if instruct_model:\n",
    "        messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "        input_text = my_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        inputs = my_tokenizer.encode(input_text, return_tensors=\"pt\").to(my_device)\n",
    "    else:\n",
    "        inputs = my_tokenizer.encode_plus(formatted_prompt, return_tensors=\"pt\").to(my_device)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def tune_generate_output(my_inputs, my_tokenizer, my_model, max_tokens=50, temp=0.3, top_p=0.9, top_k=50, penalty_score=1.2, do_sample=True, instruct_model=False):\n",
    "    \"\"\"\n",
    "    Generates a response from the model based on the input.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = my_model.generate(\n",
    "                \n",
    "        input_ids=my_inputs[\"input_ids\"],  # Pass input_ids\n",
    "        attention_mask=my_inputs[\"attention_mask\"],  # Pass attention mask\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temp,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=penalty_score,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=my_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode output and clean it up\n",
    "    output_text = my_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure safe parsing without hardcoded token removal\n",
    "    cleaned_output_text = output_text.strip()\n",
    "\n",
    "    return cleaned_output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tags', 'instruction', 'category', 'intent', 'response'],\n",
      "    num_rows: 23053\n",
      "}) Dataset({\n",
      "    features: ['tags', 'instruction', 'category', 'intent', 'response'],\n",
      "    num_rows: 1214\n",
      "}) Dataset({\n",
      "    features: ['tags', 'instruction', 'category', 'intent', 'response'],\n",
      "    num_rows: 1278\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## DATASET ##\n",
    "huggingface_dataset_name = \"bitext/Bitext-retail-banking-llm-chatbot-training-dataset\"\n",
    "\n",
    "ds = load_dataset(huggingface_dataset_name, split='train')\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_test_split = ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "train_validation_split = train_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_validation_split['train']\n",
    "validation_dataset = train_validation_split['test']\n",
    "\n",
    "print(train_dataset, validation_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11794993b4b24ab59c4119ff603a46ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Batch Size\n",
    "batch_size = 128  \n",
    "\n",
    "# PyTorch Dataset Wrapper (Fixes \"list object has no attribute 'to'\")\n",
    "class BankingDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.dataset[idx][\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.dataset[idx][\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": self.dataset[idx].get(\"response\", \"N/A\")\n",
    "        }\n",
    "\n",
    "# Tokenization with padding\n",
    "def tokenize_function(examples):\n",
    "    formatted_prompts = [\n",
    "        f\"\"\"\n",
    "        ### Instruction:\n",
    "        You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
    "\n",
    "        ### Customer Request:\n",
    "        {instruction}\n",
    "\n",
    "        ### Response:\n",
    "        \"\"\"\n",
    "        for instruction in zip(examples[\"instruction\"])\n",
    "    ]\n",
    "\n",
    "    tokenized_outputs = tokenizer(\n",
    "        formatted_prompts,\n",
    "        padding=\"longest\",  # Dynamic padding\n",
    "        truncation=True,\n",
    "        max_length=512, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return tokenized_outputs\n",
    "\n",
    "# Tokenize test dataset\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['tags', 'instruction', 'category', 'intent'])\n",
    "\n",
    "# Convert to PyTorch Dataset\n",
    "test_dataset_torch = BankingDataset(tokenized_test_dataset)\n",
    "\n",
    "# DataLoader (batch işlemi için)\n",
    "test_dataloader = DataLoader(test_dataset_torch, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model Evaluation\n",
      "--------------------------------------------------\n",
      "==========\n",
      "I need assistyance finding an ATM\n",
      "==========\n",
      "I'm here to help you find the nearest ATM in your area. To locate one, you have a few options:\n",
      "\n",
      "1. You can use a reliable mapping application like Google Maps or Apple Maps on your smartphone. Simply open the app and search for \"ATMs near me\" or \"ATMs in [your location].\" It will show you a list of nearby ATMs along with their distance and directions.\n",
      "\n",
      "2. Another option is to visit your bank's website. Most banks have an ATM locator feature on their website where you can enter your location or use your current location to find the nearest ATM belonging to your bank.\n",
      "\n",
      "3. If you're out and about and don't have access to a smartphone or computer, you can ask locals or nearby businesses for directions to the nearest ATM. They should be able to guide you to the closest one.\n",
      "\n",
      "Remember to prioritize your safety when searching for an ATM. It's a good practice to use ATMs in well-lit, public areas, and be cautious of your surroundings when making transactions. Let me know if there's anything else I can assist you with!\n",
      "==========\n",
      "### Instruction:\n",
      "    You are an AI banking assistant. Respond to the customer's request in a clear and professional manner.\n",
      "\n",
      "    ### Customer Request:\n",
      "    I need assistyance finding an ATM\n",
      "\n",
      "    ### Response:\n",
      "    \n",
      "I'm here to help you find the nearest ATMs so that you can conveniently access your funds whenever needed. To locate nearby ATMs, please follow these steps:\n",
      "\n",
      "1. Open our mobile app or visit our website at {{Company Website URL}}.\n",
      "2. Look for the \"Locations\" section or similar option within the main menu of the application/website.\n",
      "3. Tap on it to view all available ATMs near you based on distance from your current location.\n",
      "4. Filter the list by selecting specific criteria such as bank branches, drive-thru options, etc., if desired.\n",
      "5. Once filtered, check out each listed ATM address along with its operating hours (if applicable).\n",
      "6. If necessary, use the search feature provided to narrow down results further using keywords like \"{{ATM Name}}, {{{Bank Name}}}\" or any other relevant details mentioned alongside the listings.\n",
      "7. Finally, choose one convenient ATM according to availability and accessibility factors important to you.\n",
      "8. Make sure to note down the exact address and contact information associated with the selected ATM before proceeding.\n",
      "9. Double-check again just in case there might be multiple locations matching your preferences; this ensures accuracy during navigation purposes too!\n",
      "\n",
      "If you encounter difficulties while searching or require additional assistance throughout the process, feel free to reach out to us through live chat support or give me a call directly at {{Customer Support Phone Number}}. We're always ready\n",
      "==========\n",
      "I'm here to help you find the nearest ATMs so that you can conveniently access your funds whenever needed. To locate nearby ATMs, please follow these steps:\n",
      "\n",
      "1. Open our mobile app or visit our website at {{Company Website URL}}.\n",
      "2. Look for the \"Locations\" section or similar option within the main menu of the application/website.\n",
      "3. Tap on it to view all available ATMs near you based on distance from your current location.\n",
      "4. Filter the list by selecting specific criteria such as bank branches, drive-thru options, etc., if desired.\n",
      "5. Once filtered, check out each listed ATM address along with its operating hours (if applicable).\n",
      "6. If necessary, use the search feature provided to narrow down results further using keywords like \"{{ATM Name}}, {{{Bank Name}}}\" or any other relevant details mentioned alongside the listings.\n",
      "7. Finally, choose one convenient ATM according to availability and accessibility factors important to you.\n",
      "8. Make sure to note down the exact address and contact information associated with the selected ATM before proceeding.\n",
      "9. Double-check again just in case there might be multiple locations matching your preferences; this ensures accuracy during navigation purposes too!\n",
      "\n",
      "If you encounter difficulties while searching or require additional assistance throughout the process, feel free to reach out to us through live chat support or give me a call directly at {{Customer Support Phone Number}}. We're always ready\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 100\n",
    "\n",
    "sample_query = test_dataset['instruction'][sample_idx]\n",
    "sample_response = test_dataset['response'][sample_idx]\n",
    "\n",
    "if is_fine_tune_evaluation:\n",
    "    print(\"PEFT Model Evaluation\")\n",
    "    sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
    "    print(\"-\"*50)\n",
    "    #print(sample_inputs)\n",
    "\n",
    "    # peft generation\n",
    "    sample_output = tune_generate_output(sample_inputs, tokenizer, peft_model, max_tokens=300, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                        do_sample = True, instruct_model=False)\n",
    "    sample_output_wout_prompt = sample_output.split(\"Response:\")[1].strip()\n",
    "\n",
    "\n",
    "    print(\"=\"*10)\n",
    "    print(sample_query)\n",
    "    print(\"=\"*10)\n",
    "    print(sample_response)\n",
    "    print(\"=\"*10)\n",
    "    print(sample_output)\n",
    "    print(\"=\"*10)\n",
    "    print(sample_output_wout_prompt)\n",
    "\n",
    "else:\n",
    "    print(\"Base Model Evaluation\")\n",
    "    print(\"-\"*50)\n",
    "    \"\"\"\n",
    "    sample_inputs = tokenizer.encode(\"Answer: {}\".format(sample_query), return_tensors=\"pt\").to(my_device)\n",
    "    sample_output = model.generate(sample_inputs, max_new_tokens=200, temperature=0.7, top_p=0.6, top_k=50, repetition_penalty=1.2, do_sample=True)\n",
    "    sample_output = tokenizer.decode(sample_output[0])\n",
    "    \"\"\"\n",
    "\n",
    "    sample_inputs = tune_generate_chat_template(sample_query, tokenizer, my_device)\n",
    "\n",
    "    # base model generation\n",
    "    sample_output = tune_generate_output(sample_inputs, tokenizer, model, max_tokens=200, temp = 0.7, top_p = 0.6, top_k=50, penalty_score=1.2,\n",
    "                                        do_sample = True, instruct_model=False)\n",
    "    sample_output_wout_prompt = sample_output.split(\"Response:\")[1].strip()\n",
    "\n",
    "    print(\"=\"*10)\n",
    "    print(sample_query)\n",
    "    print(\"=\"*10)\n",
    "    print(sample_output)\n",
    "    print(\"=\"*10)\n",
    "    print(sample_output_wout_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [48:11<00:00, 289.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Batched inference\n",
    "generated_responses = []\n",
    "human_responses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(my_device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(my_device)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):  # Mixed precision\n",
    "            if is_fine_tune_evaluation:\n",
    "                outputs = peft_model.generate(\n",
    "                input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=200, \n",
    "                temperature=0.7, \n",
    "                top_p=0.6, \n",
    "                top_k=50, \n",
    "                repetition_penalty=1.2,\n",
    "                do_sample=True\n",
    "                )\n",
    "            else:\n",
    "                outputs = model.generate(\n",
    "                    input_ids, \n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=200, \n",
    "                    temperature=0.7, \n",
    "                    top_p=0.6, \n",
    "                    top_k=50, \n",
    "                    repetition_penalty=1.2,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "        batch_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Response parsing\n",
    "        batch_cleaned_outputs = [out.split(\"Response:\")[1].strip() if \"Response:\" in out else out for out in batch_outputs]\n",
    "\n",
    "        human_responses.extend(batch[\"labels\"])  # Ground truth\n",
    "        generated_responses.extend(batch_cleaned_outputs)\n",
    "    \n",
    "# Create DataFrame\n",
    "if is_fine_tune_evaluation:\n",
    "    df = pd.DataFrame(list(zip(human_responses, generated_responses)), \n",
    "                  columns=['human_baseline_responses', 'peft_model_responses'])\n",
    "else:\n",
    "    df = pd.DataFrame(list(zip(human_responses, generated_responses)), \n",
    "                  columns=['human_baseline_responses', 'base_model_responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_responses</th>\n",
       "      <th>peft_model_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm here to help you recover your swallowed ca...</td>\n",
       "      <td>Sure thing! I'm here to assist you with recove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm here to assist you with checking your mort...</td>\n",
       "      <td>I'm here to assist you with checking your mort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_responses  \\\n",
       "0  I'm here to help you recover your swallowed ca...   \n",
       "1  I'm here to assist you with checking your mort...   \n",
       "\n",
       "                                peft_model_responses  \n",
       "0  Sure thing! I'm here to assist you with recove...  \n",
       "1  I'm here to assist you with checking your mort...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_fine_tune_evaluation:\n",
    "    df.to_csv(peft_model_path.replace('/','__')+\".csv\", index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    df.to_csv(model_name.replace('/','__')+\".csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\citak\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sat Aug 31 02:06:36 2024) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL:\n",
      "{'rouge1': 0.4926546255851052, 'rouge2': 0.19099911354814342, 'rougeL': 0.2832873698706914, 'rougeLsum': 0.4243427394896141}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "evaluation_result = rouge.compute(\n",
    "    predictions=generated_responses,\n",
    "    references=human_responses,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "\n",
    "if is_fine_tune_evaluation:\n",
    "    print('PEFT MODEL:')\n",
    "    print(evaluation_result)\n",
    "else:\n",
    "    print('ORIGINAL MODEL:')\n",
    "    print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
